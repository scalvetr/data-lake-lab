{"paragraphs":[{"text":"%md\n\n# Obtención de fuentes de datos y enriquecimiento\n\nEn este notebook exploraremos un poco más cómo obtener datos y usarlos para enriquecer nuestro lago. Hasta el momento hemos usado comandos sencillos para la obtención de datos de forma *puntual* y registrarlos como tablas o vistas tabulares dentro de la propia plataforma Hadoop, pero ¿Cómo se realiza esto a escala?\n\nPaso por paso... Del mismo modo que obtenemos datos de forma remota mediante peticiones HTTP GET, podemos obtener dinámicamente en Spark la información de las fuentes que necesitemos. No solo obtenerla sino registrarla, registrar los resultados en entornos que no forman parte del propio Hadoop. Como puedieran ser datos provenientes de una base de datos relacional.\n\nA continuación añadiremos las dependencias necesarias para realizar algunas conexiónes fuera del entorno Hadoop. Lo primero será reiniciar nuestro interprete Spark para que pueda coger las nuevas dependencias que añadiremos a posteriori. Para ello deberéis pulsar en la ruleta que aparece en la parte superior derecha de vuestro notebook (<span class=\"setting-btn\" type=\"button\" ng-click=\"toggleSetting()\" tooltip-placement=\"bottom\" uib-tooltip=\"Interpreter binding\"> <i class=\"fa fa-cog\" ng-style=\"{color: showSetting ? '#3071A9' : 'black' }\" style=\"color: rgb(0, 0, 0);\"></i> </span>) y una vez desplegados los interpretes pulsar el boton de reinicio en el interprete Spark2 (<a ng-click=\"restartInterpreter(item)\" ng-class=\"{'inactivelink': !item.selected}\" uib-tooltip=\"Restart\"> <span class=\"glyphicon glyphicon-refresh btn-md\"></span> </a>). Podeis apoyaros en la [documentación oficial](https://zeppelin.apache.org/docs/0.8.2/usage/interpreter/overview.html) para ello.\n\nEn cualquier caso, tenéis algo más de información sobre como dotar a vuestro interpretes de estas dependencias en la [documentación oficial de Zeppelin](http://zeppelin.apache.org/docs/0.8.0/usage/interpreter/dependency_management.html)","user":"anonymous","dateUpdated":"2020-05-20T17:12:37+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Obtención de fuentes de datos y enriquecimiento</h1>\n<p>En este notebook exploraremos un poco más cómo obtener datos y usarlos para enriquecer nuestro lago. Hasta el momento hemos usado comandos sencillos para la obtención de datos de forma <em>puntual</em> y registrarlos como tablas o vistas tabulares dentro de la propia plataforma Hadoop, pero ¿Cómo se realiza esto a escala?</p>\n<p>Paso por paso&hellip; Del mismo modo que obtenemos datos de forma remota mediante peticiones HTTP GET, podemos obtener dinámicamente en Spark la información de las fuentes que necesitemos. No solo obtenerla sino registrarla, registrar los resultados en entornos que no forman parte del propio Hadoop. Como puedieran ser datos provenientes de una base de datos relacional.</p>\n<p>A continuación añadiremos las dependencias necesarias para realizar algunas conexiónes fuera del entorno Hadoop. Lo primero será reiniciar nuestro interprete Spark para que pueda coger las nuevas dependencias que añadiremos a posteriori. Para ello deberéis pulsar en la ruleta que aparece en la parte superior derecha de vuestro notebook (<span class=\"setting-btn\" type=\"button\" ng-click=\"toggleSetting()\" tooltip-placement=\"bottom\" uib-tooltip=\"Interpreter binding\"> <i class=\"fa fa-cog\" ng-style=\"{color: showSetting ? '#3071A9' : 'black' }\" style=\"color: rgb(0, 0, 0);\"></i> </span>) y una vez desplegados los interpretes pulsar el boton de reinicio en el interprete Spark2 (<a ng-click=\"restartInterpreter(item)\" ng-class=\"{'inactivelink': !item.selected}\" uib-tooltip=\"Restart\"> <span class=\"glyphicon glyphicon-refresh btn-md\"></span> </a>). Podeis apoyaros en la <a href=\"https://zeppelin.apache.org/docs/0.8.2/usage/interpreter/overview.html\">documentación oficial</a> para ello.</p>\n<p>En cualquier caso, tenéis algo más de información sobre como dotar a vuestro interpretes de estas dependencias en la <a href=\"http://zeppelin.apache.org/docs/0.8.0/usage/interpreter/dependency_management.html\">documentación oficial de Zeppelin</a></p>\n"}]},"apps":[],"jobName":"paragraph_1588693933109_-445130281","id":"20200505-155213_1398147600","dateCreated":"2020-05-05T15:52:13+0000","dateStarted":"2020-05-20T17:12:37+0000","dateFinished":"2020-05-20T17:12:37+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:16753"},{"text":"%spark2.dep\r\nz.reset()\r\n// Y añadimos el conector MySQL al contexto Spark\r\nz.load(\"/usr/share/java/mysql-connector-java.jar\")","user":"anonymous","dateUpdated":"2020-05-20T17:12:58+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res0: org.apache.zeppelin.dep.Dependency = org.apache.zeppelin.dep.Dependency@5cbd6e34\n"}]},"apps":[],"jobName":"paragraph_1588699733811_-265770025","id":"20200505-172853_1114743249","dateCreated":"2020-05-05T17:28:53+0000","dateStarted":"2020-05-20T17:12:58+0000","dateFinished":"2020-05-20T17:13:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16754"},{"text":"%md\n\nCon este sencillo comando acabamos de añadir a nuestro entorno el código necesario para establecer conexiones con una base de datos MySQL. Deberíais obtener una salida del tipo:\n\n> res0: org.apache.zeppelin.dep.Dependency = org.apache.zeppelin.dep.Dependency@xxx\n\nUna vez hecho esto, podéis probar a realizar una conexión a un entorno local e intentar obtener, por ejemplo, las políticas en vigor en la instalación de [Apache Ranger local](http://localhost:6080/login.jsp).","user":"anonymous","dateUpdated":"2020-05-20T17:14:11+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Con este sencillo comando acabamos de añadir a nuestro entorno el código necesario para establecer conexiones con una base de datos MySQL. Deberíais obtener una salida del tipo:</p>\n<blockquote><p>res0: org.apache.zeppelin.dep.Dependency = org.apache.zeppelin.dep.Dependency@xxx</p>\n</blockquote>\n<p>Una vez hecho esto, podéis probar a realizar una conexión a un entorno local e intentar obtener, por ejemplo, las políticas en vigor en la instalación de <a href=\"http://localhost:6080/login.jsp\">Apache Ranger local</a>.</p>\n"}]},"apps":[],"jobName":"paragraph_1588777615645_-1025046415","id":"20200506-150655_316070731","dateCreated":"2020-05-06T15:06:55+0000","dateStarted":"2020-05-20T17:14:11+0000","dateFinished":"2020-05-20T17:14:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16755"},{"text":"%spark2\n\n// Definamos como sería nuestra consulta SQL\nval sql=\"\"\"select * from x_policy\"\"\"\n\n// Y a continuación realizaremos la lectura, no del almacenamiento del datalake si no de la base de datos relacional\nval df = spark.read\n  .format(\"jdbc\")\n  .option(\"url\", \"jdbc:mysql://localhost:3306/ranger\")\n  .option(\"driver\", \"com.mysql.jdbc.Driver\")\n  .option(\"dbtable\",  s\"( $sql ) t\")\n  .option(\"user\", \"root\")\n  .option(\"password\", \"hortonworks1\")\n  .load()","user":"anonymous","dateUpdated":"2020-05-20T17:14:15+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"sql: String = select * from x_policy\ndf: org.apache.spark.sql.DataFrame = [id: bigint, guid: string ... 14 more fields]\n"}]},"apps":[],"jobName":"paragraph_1588776622137_2117438968","id":"20200506-145022_1964992444","dateCreated":"2020-05-06T14:50:22+0000","dateStarted":"2020-05-20T17:14:16+0000","dateFinished":"2020-05-20T17:15:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16756"},{"text":"%md\n\nVaya, no se ve nada. Esto se debe a que Spark cómo sabeis es laxo en su ejecución y hasta que no le pidamos una acción no nos mostrará nada...","user":"anonymous","dateUpdated":"2020-05-06T15:11:59+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Vaya, no se ve nada. Esto se debe a que Spark cómo sabeis es laxo en su ejecución y hasta que no le pidamos una acción no nos mostrará nada&hellip;</p>\n"}]},"apps":[],"jobName":"paragraph_1588777875388_1924254052","id":"20200506-151115_923341395","dateCreated":"2020-05-06T15:11:15+0000","dateStarted":"2020-05-06T15:11:59+0000","dateFinished":"2020-05-06T15:11:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16757"},{"text":"%spark2\n\ndf.show()\n","user":"anonymous","dateUpdated":"2020-05-06T15:12:10+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+--------------------+-------------------+-------------------+-----------+---------+-------+-------+--------------------+-----------+--------------------+--------------------+----------+----------------+--------------+---------------+\n| id|                guid|        create_time|        update_time|added_by_id|upd_by_id|version|service|                name|policy_type|         description|  resource_signature|is_enabled|is_audit_enabled|policy_options|policy_priority|\n+---+--------------------+-------------------+-------------------+-----------+---------+-------+-------+--------------------+-----------+--------------------+--------------------+----------+----------------+--------------+---------------+\n|  1|c45f46ec-3346-462...|2018-11-29 18:52:21|2020-05-02 09:30:47|          5|        1|      4|      1|          all - path|          0|Policy for all - ...|dfe81e379022be6ca...|      true|            true|          null|              0|\n|  2|eff1e4dd-dd58-451...|2018-11-29 18:52:22|2018-11-29 18:52:22|          5|        5|      1|      1|      kms-audit-path|          0|Policy for kms-au...|b349a238ab1cea962...|      true|            true|          null|              0|\n|  3|a62ae2a3-ba25-45a...|2018-11-29 18:55:05|2018-11-29 18:55:05|          5|        5|      1|      2|all - topology, s...|          0|Policy for all - ...|bcdaa6d5b59d1ff76...|      true|            true|          null|              0|\n|  4|862b3c72-9a67-486...|2018-11-29 18:55:23|2018-11-29 18:55:23|          5|        5|      1|      3|         all - queue|          0|Policy for all - ...|ed9b305757e6a786d...|      true|            true|          null|              0|\n|  5|301a4700-fa9d-47c...|2018-11-29 18:55:23|2018-11-29 18:55:23|          5|        5|      1|      3|Service Check Use...|          0|Policy for Servic...|0bfff329359d64c93...|      true|            true|          null|              0|\n|  6|b4b3ffc1-3a52-41b...|2018-11-29 18:59:35|2018-11-29 18:59:35|          5|        5|      1|      4|all - entity-type...|          0|Policy for all - ...|cfa28062e015d86cc...|      true|            true|          null|              0|\n|  7|b6725240-ca8d-441...|2018-11-29 18:59:35|2018-11-29 18:59:35|          5|        5|      1|      4|all - relationshi...|          0|Policy for all - ...|61c3a6849c77a1613...|      true|            true|          null|              0|\n|  8|c117ef9b-fa5f-441...|2018-11-29 18:59:36|2018-11-29 18:59:36|          5|        5|      1|      4| all - atlas-service|          0|Policy for all - ...|18a7b8a5b4f223bf7...|      true|            true|          null|              0|\n|  9|f672f263-9bd2-476...|2018-11-29 18:59:36|2018-11-29 18:59:36|          5|        5|      1|      4|all - type-catego...|          0|Policy for all - ...|c200eb7817dbc41a8...|      true|            true|          null|              0|\n| 10|e13295cd-ef6f-45e...|2018-11-29 19:03:45|2018-11-29 19:03:45|          5|        5|      1|      5|   all - hiveservice|          0|Policy for all - ...|c052be6821aab8eaa...|      true|            true|          null|              0|\n| 11|53b53161-d8c3-408...|2018-11-29 19:03:45|2018-11-29 19:03:45|          5|        5|      1|      5|        all - global|          0|Policy for all - ...|7b67e1ab9cbc3011d...|      true|            true|          null|              0|\n| 12|ef10f46d-0c91-4f0...|2018-11-29 19:03:46|2018-11-29 19:03:46|          5|        5|      1|      5|           all - url|          0|Policy for all - url|a6a328c623a4eb15d...|      true|            true|          null|              0|\n| 13|52b9596c-dac6-49d...|2018-11-29 19:03:46|2018-11-29 19:03:46|          5|        5|      1|      5|all - database, t...|          0|Policy for all - ...|ffd181600c642189e...|      true|            true|          null|              0|\n| 14|f262483b-d57a-409...|2018-11-29 19:03:46|2018-11-29 19:03:46|          5|        5|      1|      5| all - database, udf|          0|Policy for all - ...|a9a4ce765b10e4723...|      true|            true|          null|              0|\n+---+--------------------+-------------------+-------------------+-----------+---------+-------+-------+--------------------+-----------+--------------------+--------------------+----------+----------------+--------------+---------------+\n\n"}]},"apps":[],"jobName":"paragraph_1588776592765_1234352230","id":"20200506-144952_472064473","dateCreated":"2020-05-06T14:49:52+0000","dateStarted":"2020-05-06T15:12:10+0000","dateFinished":"2020-05-06T15:12:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16758"},{"text":"%spark2\n\ndf.where(\"is_enabled == true\").count()","user":"anonymous","dateUpdated":"2020-05-06T15:13:14+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res5: Long = 14\n"}]},"apps":[],"jobName":"paragraph_1588777965777_-174971951","id":"20200506-151245_1818456997","dateCreated":"2020-05-06T15:12:45+0000","dateStarted":"2020-05-06T15:13:14+0000","dateFinished":"2020-05-06T15:13:19+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16759"},{"text":"%md\n\nNo solo podemos leer y computar acciones sino también escribir. \n\nEsto cobra especial relevancia cuando queremos trabajar con datos voluminosos pero el resultado de nuestro proceso no lo es tanto, gracias a esto podemos devolver los resultados a un entorno donde los usuarios que vayan a consumir los datos se sientan más cómodos o estos resultados vayan a prestar servicio a una aplicación concreta.\n\nSigamos con el ejemplo de los sensores de Santander. En este caso vamos a obtener la localización de algunos de los sensores para los que estamos obteniendo los datos.\n","user":"anonymous","dateUpdated":"2020-05-20T17:16:35+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>No solo podemos leer y computar acciones sino también escribir.</p>\n<p>Esto cobra especial relevancia cuando queremos trabajar con datos voluminosos pero el resultado de nuestro proceso no lo es tanto, gracias a esto podemos devolver los resultados a un entorno donde los usuarios que vayan a consumir los datos se sientan más cómodos o estos resultados vayan a prestar servicio a una aplicación concreta.</p>\n<p>Sigamos con el ejemplo de los sensores de Santander. En este caso vamos a obtener la localización de algunos de los sensores para los que estamos obteniendo los datos.</p>\n"}]},"apps":[],"jobName":"paragraph_1588778116411_-1965269508","id":"20200506-151516_385687813","dateCreated":"2020-05-06T15:15:16+0000","dateStarted":"2020-05-20T17:16:35+0000","dateFinished":"2020-05-20T17:16:35+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16760"},{"text":"%spark2\n\n// Recordad que Spark no deja de ser un proyecto Scala como cualquier otro con lo que\n// podemos programar lo que nos haga falta y paralelizarlo en lo ejecutores disponibles\nimport scala.io.Source._\nimport org.apache.spark.sql.Dataset\n\nvar res = fromURL(\"https://raw.githubusercontent.com/IraitzM/Santander/master/location.csv\").mkString.stripMargin.lines.toList\nval csvData: Dataset[String] = spark.sparkContext.parallelize(res).toDS()\nval location = spark.read.option(\"header\", true).option(\"inferSchema\",true).csv(csvData)\nlocation.printSchema()","user":"anonymous","dateUpdated":"2020-05-06T15:25:21+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- lon: double (nullable = true)\n |-- lat: double (nullable = true)\n |-- sensor: integer (nullable = true)\n\nimport scala.io.Source._\nimport org.apache.spark.sql.Dataset\nres: List[String] = List(lon,lat,sensor, -3.823591450918023,43.4593294563689,2012, -3.7954695353964687,43.463067249052074,2037, -3.79358146194165,43.46284625642153,2032, -3.7965489376875947,43.46336588126894,2033, -3.796755697161408,43.46404876827053,2034, -3.7983820118645273,43.46231760108342,2030, -3.8006759663261986,43.4619143755439,2043, -3.8108382488544974,43.461806746502354,2016, -3.8118337178388275,43.46155656663748,2040, -3.8123620473656814,43.46177792206734,2015, -3.811675316244493,43.46209794982114,2014, -3.810254302825353,43.461585778148,2017, -3.8092627101024648,43.461385701260106,2019, -3.7995263327061872,43.46257073037124,2029, -3.799856082256003,43.46217223447628,2028, -3.803732514814816,43.46178487983374,2026, ..."}]},"apps":[],"jobName":"paragraph_1588694073505_-475394332","id":"20200505-155433_1004017947","dateCreated":"2020-05-05T15:54:33+0000","dateStarted":"2020-05-06T15:25:22+0000","dateFinished":"2020-05-06T15:25:37+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16761"},{"text":"%spark2\n\nlocation.show()\n","user":"anonymous","dateUpdated":"2020-05-05T17:06:54+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------------------+------------------+------+\n|                lon|               lat|sensor|\n+-------------------+------------------+------+\n| -3.823591450918023|  43.4593294563689|  2012|\n|-3.7954695353964687|43.463067249052074|  2037|\n|  -3.79358146194165| 43.46284625642153|  2032|\n|-3.7965489376875947| 43.46336588126894|  2033|\n| -3.796755697161408| 43.46404876827053|  2034|\n|-3.7983820118645273| 43.46231760108342|  2030|\n|-3.8006759663261986|  43.4619143755439|  2043|\n|-3.8108382488544974|43.461806746502354|  2016|\n|-3.8118337178388275| 43.46155656663748|  2040|\n|-3.8123620473656814| 43.46177792206734|  2015|\n| -3.811675316244493| 43.46209794982114|  2014|\n| -3.810254302825353|   43.461585778148|  2017|\n|-3.8092627101024648|43.461385701260106|  2019|\n|-3.7995263327061872| 43.46257073037124|  2029|\n| -3.799856082256003| 43.46217223447628|  2028|\n| -3.803732514814816| 43.46178487983374|  2026|\n| -3.820468915658142|43.460234327330596|  2013|\n| -3.806259943588895|43.461559987147204|  2024|\n|-3.8176878393963745|43.455572004696336|  1010|\n|-3.8208669437179035|  43.4544777123065|  1007|\n+-------------------+------------------+------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1588697996976_-2096328861","id":"20200505-165956_1235195210","dateCreated":"2020-05-05T16:59:56+0000","dateStarted":"2020-05-05T17:06:54+0000","dateFinished":"2020-05-05T17:07:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16762"},{"text":"%md\n\nYa tenemos nuestro frame traído de una ubicación remota y hemos visto cómo poder realizar operaciones con Spark en el cluster. Pero ¿y si quisieramos llevar los resultado a otro destino? ¿Un destino tabular que no fuera parte de la infraestructura Hadoop?\n\nImaginémonos que nuestra capa batch a realizado un cálculo voluminoso, obteniendo información ya refinada y queremos llevarnos estos resultados a una capa de servicio. Por ejemplo una base de datos relacional al uso como hemos indicado antes.\n","user":"anonymous","dateUpdated":"2020-05-20T17:18:24+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Ya tenemos nuestro frame traído de una ubicación remota y hemos visto cómo poder realizar operaciones con Spark en el cluster. Pero ¿y si quisieramos llevar los resultado a otro destino? ¿Un destino tabular que no fuera parte de la infraestructura Hadoop?</p>\n<p>Imaginémonos que nuestra capa batch a realizado un cálculo voluminoso, obteniendo información ya refinada y queremos llevarnos estos resultados a una capa de servicio. Por ejemplo una base de datos relacional al uso como hemos indicado antes.</p>\n"}]},"apps":[],"jobName":"paragraph_1588698429027_-1070311413","id":"20200505-170709_1578929109","dateCreated":"2020-05-05T17:07:09+0000","dateStarted":"2020-05-20T17:18:24+0000","dateFinished":"2020-05-20T17:18:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16763"},{"text":"%spark2\n\n// Indicamos los datos de nuestros sensores por un lado\nval data = spark.sql(\"SELECT * FROM santander_json\")\n\n// Los juntamos con los datos de ubicación de los sensores por otro\nval joindf = data.join(location, location(\"sensor\") === data(\"idsensor\"))\njoindf.show()","user":"anonymous","dateUpdated":"2020-05-06T17:22:42+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-----+--------+----------+---------+--------------------+-------------------+------------------+------+\n|carga|idsensor|intensidad|ocupacion|               fecha|                lon|               lat|sensor|\n+-----+--------+----------+---------+--------------------+-------------------+------------------+------+\n|   75|    2012|       480|       70|2020-05-03T15:40:00Z| -3.823591450918023|  43.4593294563689|  2012|\n|    0|    2037|         0|        0|2020-05-03T15:40:00Z|-3.7954695353964687|43.463067249052074|  2037|\n|    0|    2032|         0|        0|2020-05-03T15:40:00Z|  -3.79358146194165| 43.46284625642153|  2032|\n|    0|    2033|         0|        0|2020-05-03T15:40:00Z|-3.7965489376875947| 43.46336588126894|  2033|\n|    0|    2034|         0|        0|2020-05-03T15:40:00Z| -3.796755697161408| 43.46404876827053|  2034|\n|    0|    2030|         0|        0|2020-05-03T15:40:00Z|-3.7983820118645273| 43.46231760108342|  2030|\n|    0|    2043|         0|        0|2020-05-03T15:40:00Z|-3.8006759663261986|  43.4619143755439|  2043|\n|    4|    2016|        60|        2|2020-05-03T15:40:00Z|-3.8108382488544974|43.461806746502354|  2016|\n|    4|    2040|        60|        0|2020-05-03T15:40:00Z|-3.8118337178388275| 43.46155656663748|  2040|\n|   28|    2015|       734|       10|2020-05-03T15:40:00Z|-3.8123620473656814| 43.46177792206734|  2015|\n|    7|    2014|        60|        2|2020-05-03T15:40:00Z| -3.811675316244493| 43.46209794982114|  2014|\n|    2|    2017|        60|        0|2020-05-03T15:40:00Z| -3.810254302825353|   43.461585778148|  2017|\n|    6|    2019|       171|        2|2020-05-03T15:40:00Z|-3.8092627101024648|43.461385701260106|  2019|\n|    0|    2029|         0|        0|2020-05-03T15:40:00Z|-3.7995263327061872| 43.46257073037124|  2029|\n|    4|    2028|       120|        1|2020-05-03T15:40:00Z| -3.799856082256003| 43.46217223447628|  2028|\n|    5|    2026|       140|        0|2020-05-03T15:40:00Z| -3.803732514814816| 43.46178487983374|  2026|\n|    0|    2013|         0|        0|2020-05-03T15:40:00Z| -3.820468915658142|43.460234327330596|  2013|\n|    6|    2024|       120|        1|2020-05-03T15:40:00Z| -3.806259943588895|43.461559987147204|  2024|\n|   36|    1010|      1685|       11|2020-05-03T15:40:00Z|-3.8176878393963745|43.455572004696336|  1010|\n|   53|    1007|      1901|       30|2020-05-03T15:40:00Z|-3.8208669437179035|  43.4544777123065|  1007|\n+-----+--------+----------+---------+--------------------+-------------------+------------------+------+\nonly showing top 20 rows\n\ndata: org.apache.spark.sql.DataFrame = [carga: string, idsensor: string ... 3 more fields]\njoindf: org.apache.spark.sql.DataFrame = [carga: string, idsensor: string ... 6 more fields]\n"}]},"apps":[],"jobName":"paragraph_1588778517550_-1297570672","id":"20200506-152157_991765217","dateCreated":"2020-05-06T15:21:57+0000","dateStarted":"2020-05-06T15:25:45+0000","dateFinished":"2020-05-06T15:26:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16764"},{"text":"%md\n\nEs importante en este punto que recordéis que Spark es vago en su evaluación con lo que cada vez que pedimos realizar una ación sobre el dataframe objeto, este es evaluado desde el origen volviendo a recuperar la información desde el principio (en nuestro caso tanto de los ficheros del HDFS como del GitHub donde se obtienen las ubicaciones). Esto quiere decir que si cualquiera de los origenes de datos cambia, el resultado también lo hará.\n\nPodeis explorar el plan de ejecución como se os muestra a continuación o enredar más en el Spark UI para entender el flujo de datos y las ineficiencias de estas operaciones cuando las movemos a escalas Big Data, dado que los datos normalizados generan transmisión de datos entre nodos que generan mucho tráfico de red en el cluster.","user":"anonymous","dateUpdated":"2020-05-20T17:20:26+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Es importante en este punto que recordéis que Spark es vago en su evaluación con lo que cada vez que pedimos realizar una ación sobre el dataframe objeto, este es evaluado desde el origen volviendo a recuperar la información desde el principio (en nuestro caso tanto de los ficheros del HDFS como del GitHub donde se obtienen las ubicaciones). Esto quiere decir que si cualquiera de los origenes de datos cambia, el resultado también lo hará.</p>\n<p>Podeis explorar el plan de ejecución como se os muestra a continuación o enredar más en el Spark UI para entender el flujo de datos y las ineficiencias de estas operaciones cuando las movemos a escalas Big Data, dado que los datos normalizados generan transmisión de datos entre nodos que generan mucho tráfico de red en el cluster.</p>\n"}]},"apps":[],"jobName":"paragraph_1588778920889_1843413519","id":"20200506-152840_1757782758","dateCreated":"2020-05-06T15:28:40+0000","dateStarted":"2020-05-20T17:20:26+0000","dateFinished":"2020-05-20T17:20:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16765"},{"text":"%spark2\n\njoindf.explain()\n","user":"anonymous","dateUpdated":"2020-05-06T15:34:04+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{"0":{"graph":{"mode":"table","height":502.667,"optionOpen":false}}},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"== Physical Plan ==\n*(2) BroadcastHashJoin [cast(idsensor#274 as int)], [sensor#269], Inner, BuildLeft\n:- BroadcastExchange HashedRelationBroadcastMode(List(cast(cast(input[1, string, true] as int) as bigint)))\n:  +- *(1) Project [carga#273, idsensor#274, intensidad#275, ocupacion#276, fecha#277]\n:     +- *(1) Filter isnotnull(idsensor#274)\n:        +- *(1) FileScan parquet default.santander_json[carga#273,idsensor#274,intensidad#275,ocupacion#276,fecha#277] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://sandbox-hdp.hortonworks.com:8020/apps/spark/warehouse/santander_json], PartitionFilters: [], PushedFilters: [IsNotNull(idsensor)], ReadSchema: struct<carga:string,idsensor:string,intensidad:string,ocupacion:string,fecha:string>\n+- *(2) Filter isnotnull(sensor#269)\n   +- Scan ExistingRDD[lon#267,lat#268,sensor#269]\n"}]},"apps":[],"jobName":"paragraph_1588779057327_-674371819","id":"20200506-153057_831251177","dateCreated":"2020-05-06T15:30:57+0000","dateStarted":"2020-05-06T15:31:13+0000","dateFinished":"2020-05-06T15:31:15+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16766"},{"text":"%md\n\nPor último, tal y como hemos dicho, podemos registrar esta información en nuestra base de datos relacional destino.","user":"anonymous","dateUpdated":"2020-05-06T15:39:05+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Por último, tal y como hemos dicho, podemos registrar esta información en nuestra base de datos relacional destino.</p>\n"}]},"apps":[],"jobName":"paragraph_1588779268033_-1886635672","id":"20200506-153428_825761172","dateCreated":"2020-05-06T15:34:28+0000","dateStarted":"2020-05-06T15:39:05+0000","dateFinished":"2020-05-06T15:39:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16767"},{"text":"%spark2\n\n// Saving data to a JDBC source\njoindf.write\n  .format(\"jdbc\")\n  .option(\"url\", \"jdbc:mysql://localhost:3306/ranger\")\n  .option(\"driver\", \"com.mysql.jdbc.Driver\")\n  .option(\"dbtable\",  s\"santander_location\")\n  .option(\"user\", \"root\")\n  .option(\"password\", \"hortonworks1\")\n  .save()","user":"anonymous","dateUpdated":"2020-05-06T15:42:10+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1588779263339_487556891","id":"20200506-153423_1169884006","dateCreated":"2020-05-06T15:34:23+0000","dateStarted":"2020-05-06T15:42:10+0000","dateFinished":"2020-05-06T15:42:35+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16768"},{"text":"%md\n\n# Apache Sqoop\n\nPrecisamente cuando hacemos este tipo de operaciones a escala, estar leyendo constantemente de fuentes relacionales la misma información puede darnos problemas llegando a generar denegaciones de servicio en la fuente.\n\nPara realizar este tipo de lecturas batch en paralelo y a escala, trayéndonos la información al HDFS de forma que sea más sencillo consumirla y operar de forma masiva se creó Apache Sqoop. Es un proyecto que se contecta por JDBC a una base de datos y la ataca con tantos ejecutores o mappers como queramos definir.\n\nHaremos un par de operaciones sencillas para que veais como podeís usarlo aunque Zeppelin no suele ser el interfaz ideal para lanzar este tipo de operaciones. Lo primero será ver que opciones nos dá esta herramienta.","user":"anonymous","dateUpdated":"2020-05-20T17:22:24+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Apache Sqoop</h1>\n<p>Precisamente cuando hacemos este tipo de operaciones a escala, estar leyendo constantemente de fuentes relacionales la misma información puede darnos problemas llegando a generar denegaciones de servicio en la fuente.</p>\n<p>Para realizar este tipo de lecturas batch en paralelo y a escala, trayéndonos la información al HDFS de forma que sea más sencillo consumirla y operar de forma masiva se creó Apache Sqoop. Es un proyecto que se contecta por JDBC a una base de datos y la ataca con tantos ejecutores o mappers como queramos definir.</p>\n<p>Haremos un par de operaciones sencillas para que veais como podeís usarlo aunque Zeppelin no suele ser el interfaz ideal para lanzar este tipo de operaciones. Lo primero será ver que opciones nos dá esta herramienta.</p>\n"}]},"apps":[],"jobName":"paragraph_1588779778492_-385838683","id":"20200506-154258_1374259086","dateCreated":"2020-05-06T15:42:58+0000","dateStarted":"2020-05-20T17:22:24+0000","dateFinished":"2020-05-20T17:22:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16769"},{"text":"%sh\n\nsqoop help","user":"anonymous","dateUpdated":"2020-05-06T15:47:45+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"database":"string","tableName":"string","isTemporary":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Warning: /usr/hdp/3.0.1.0-187/accumulo does not exist! Accumulo imports will fail.\nPlease set $ACCUMULO_HOME to the root of your Accumulo installation.\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/usr/hdp/3.0.1.0-187/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/usr/hdp/3.0.1.0-187/hive/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n20/05/06 15:47:59 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7.3.0.1.0-187\nusage: sqoop COMMAND [ARGS]\n\nAvailable commands:\n  codegen            Generate code to interact with database records\n  create-hive-table  Import a table definition into Hive\n  eval               Evaluate a SQL statement and display the results\n  export             Export an HDFS directory to a database table\n  help               List available commands\n  import             Import a table from a database to HDFS\n  import-all-tables  Import tables from a database to HDFS\n  import-mainframe   Import datasets from a mainframe server to HDFS\n  job                Work with saved jobs\n  list-databases     List available databases on a server\n  list-tables        List available tables in a database\n  merge              Merge results of incremental imports\n  metastore          Run a standalone Sqoop metastore\n  version            Display version information\n\nSee 'sqoop help COMMAND' for information on a specific command.\n"}]},"apps":[],"jobName":"paragraph_1588779437839_1897408899","id":"20200506-153717_633584461","dateCreated":"2020-05-06T15:37:17+0000","dateStarted":"2020-05-06T15:47:45+0000","dateFinished":"2020-05-06T15:47:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16770"},{"text":"%sh\n\n# Con list tables podemos interrogar una base de datos para ver qué nos ofrece\nsqoop list-tables \\\n--connect jdbc:mysql://localhost:3306/ranger \\\n--username root \\\n--password hortonworks1","user":"anonymous","dateUpdated":"2020-05-06T15:51:06+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Warning: /usr/hdp/3.0.1.0-187/accumulo does not exist! Accumulo imports will fail.\nPlease set $ACCUMULO_HOME to the root of your Accumulo installation.\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/usr/hdp/3.0.1.0-187/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/usr/hdp/3.0.1.0-187/hive/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n20/05/06 15:51:16 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7.3.0.1.0-187\n20/05/06 15:51:17 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.\n20/05/06 15:51:17 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.\nWed May 06 15:51:18 UTC 2020 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.\nsantander_location\nvx_trx_log\nx_access_type_def\nx_access_type_def_grants\nx_asset\nx_audit_map\nx_auth_sess\nx_context_enricher_def\nx_cred_store\nx_data_hist\nx_datamask_type_def\nx_db_base\nx_db_version_h\nx_enum_def\nx_enum_element_def\nx_group\nx_group_groups\nx_group_module_perm\nx_group_users\nx_modules_master\nx_perm_map\nx_plugin_info\nx_policy\nx_policy_condition_def\nx_policy_export_audit\nx_policy_item\nx_policy_item_access\nx_policy_item_condition\nx_policy_item_datamask\nx_policy_item_group_perm\nx_policy_item_rowfilter\nx_policy_item_user_perm\nx_policy_label\nx_policy_label_map\nx_policy_resource\nx_policy_resource_map\nx_portal_user\nx_portal_user_role\nx_resource\nx_resource_def\nx_service\nx_service_config_def\nx_service_config_map\nx_service_def\nx_service_resource\nx_service_resource_element\nx_service_resource_element_val\nx_service_version_info\nx_tag\nx_tag_attr\nx_tag_attr_def\nx_tag_def\nx_tag_resource_map\nx_trx_log\nx_ugsync_audit_info\nx_user\nx_user_module_perm\nxa_access_audit\n"}]},"apps":[],"jobName":"paragraph_1588698845324_109890235","id":"20200505-171405_108420722","dateCreated":"2020-05-05T17:14:05+0000","dateStarted":"2020-05-06T15:51:06+0000","dateFinished":"2020-05-06T15:51:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16771"},{"text":"%md\n\nVaya, hay una que nos suena ¿verdad? Es precisamente la que hemos construido con Spark y hemos guardado en MySQL (sin tocar el HDFS para nada). \n\nPodemos interrogar esta tabla también con Sqoop para ver qué información tiene, gracias al comando *sqoop eval*.\n","user":"anonymous","dateUpdated":"2020-05-20T17:23:22+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Vaya, hay una que nos suena ¿verdad? Es precisamente la que hemos construido con Spark y hemos guardado en MySQL (sin tocar el HDFS para nada).</p>\n<p>Podemos interrogar esta tabla también con Sqoop para ver qué información tiene, gracias al comando <em>sqoop eval</em>.</p>\n"}]},"apps":[],"jobName":"paragraph_1588780299985_1881725999","id":"20200506-155139_345624720","dateCreated":"2020-05-06T15:51:39+0000","dateStarted":"2020-05-20T17:23:22+0000","dateFinished":"2020-05-20T17:23:22+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16772"},{"text":"%sh\n\nsqoop eval --connect jdbc:mysql://localhost:3306/ranger \\\n--username root \\\n--password hortonworks1 \\\n--query \"select * from santander_location LIMIT 10\"","user":"anonymous","dateUpdated":"2020-05-06T15:54:50+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Warning: /usr/hdp/3.0.1.0-187/accumulo does not exist! Accumulo imports will fail.\nPlease set $ACCUMULO_HOME to the root of your Accumulo installation.\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/usr/hdp/3.0.1.0-187/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/usr/hdp/3.0.1.0-187/hive/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n20/05/06 15:55:03 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7.3.0.1.0-187\n20/05/06 15:55:04 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.\n20/05/06 15:55:04 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.\nWed May 06 15:55:06 UTC 2020 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n| carga                | idsensor             | intensidad           | ocupacion            | fecha                | lon                  | lat                  | sensor      | \n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n| 0                    | 5018                 | 0                    | 0                    | 2020-05-03T15:40:00Z | -3.852367349203512   | 43.43939009220081    | 5018        | \n| 0                    | 5019                 | 4                    | 0                    | 2020-05-03T15:40:00Z | -3.8535960261265814  | 43.43871460556206    | 5019        | \n| 0                    | 5020                 | 0                    | 0                    | 2020-05-03T15:40:00Z | -3.8529779352275524  | 43.43852112240097    | 5020        | \n| 0                    | 5022                 | 0                    | 0                    | 2020-05-03T15:40:00Z | -3.853470891585292   | 43.4384274006912     | 5022        | \n| 0                    | 5021                 | 0                    | 0                    | 2020-05-03T15:40:00Z | -3.853386494363904   | 43.438401017563784   | 5021        | \n| 0                    | 5023                 | 0                    | 0                    | 2020-05-03T15:40:00Z | -3.853261488002455   | 43.43812281583237    | 5023        | \n| 0                    | 5024                 | 0                    | 0                    | 2020-05-03T15:40:00Z | -3.8533013753435825  | 43.43805948778976    | 5024        | \n| 0                    | 5025                 | 0                    | 0                    | 2020-05-03T15:40:00Z | -3.854532389535859   | 43.4385185206503     | 5025        | \n| 0                    | 5026                 | 0                    | 0                    | 2020-05-03T15:40:00Z | -3.8544887965245227  | 43.43858187680819    | 5026        | \n| 2                    | 5027                 | 4                    | 2                    | 2020-05-03T15:40:00Z | -3.8546149183014533  | 43.4388510644926     | 5027        | \n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"}]},"apps":[],"jobName":"paragraph_1588698860809_1497489948","id":"20200505-171420_195758747","dateCreated":"2020-05-05T17:14:20+0000","dateStarted":"2020-05-06T15:54:51+0000","dateFinished":"2020-05-06T15:55:07+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16773"},{"text":"%md\n\nParece que está bien esa tabla, pero ante la duda vamos a traernos esa información al HDFS. Esto lo podemos hacer mediante el uso de *sqoop import*.\n\nEs un comando curioso. Dado que desconoce cuánta información hay y cómo está distribuida, necesita que nosotros le indiquemos que columna puede usar para realizar la división de la tabla en trozos.\n\nCon esta información veréis cómo primero consulta por el MAX(columna) y MIN(columna) para luego así poder dividir los intervalos en los *M* trozos indicados. De este modo, cada proceso trabajador lanzará sus consultas limitando en el WHERE el número de filas que le correspondan acorde a los valores de la columna seleccionada.","user":"anonymous","dateUpdated":"2020-05-20T17:28:09+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Parece que está bien esa tabla, pero ante la duda vamos a traernos esa información al HDFS. Esto lo podemos hacer mediante el uso de <em>sqoop import</em>.</p>\n<p>Es un comando curioso. Dado que desconoce cuánta información hay y cómo está distribuida, necesita que nosotros le indiquemos que columna puede usar para realizar la división de la tabla en trozos.</p>\n<p>Con esta información veréis cómo primero consulta por el MAX(columna) y MIN(columna) para luego así poder dividir los intervalos en los <em>M</em> trozos indicados. De este modo, cada proceso trabajador lanzará sus consultas limitando en el WHERE el número de filas que le correspondan acorde a los valores de la columna seleccionada.</p>\n"}]},"apps":[],"jobName":"paragraph_1588780515474_-1742161649","id":"20200506-155515_2127459226","dateCreated":"2020-05-06T15:55:15+0000","dateStarted":"2020-05-20T17:28:09+0000","dateFinished":"2020-05-20T17:28:09+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16774"},{"text":"%sh\n\nsqoop import --connect jdbc:mysql://localhost:3306/ranger \\\n--username root \\\n--password hortonworks1 \\\n--split-by sensor \\\n--table \"santander_location\" \\\n--target-dir \"/tmp/mysql_import\"","user":"anonymous","dateUpdated":"2020-05-06T16:03:55+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"INCOMPLETE","msg":[{"type":"TEXT","data":"Warning: /usr/hdp/3.0.1.0-187/accumulo does not exist! Accumulo imports will fail.\nPlease set $ACCUMULO_HOME to the root of your Accumulo installation.\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/usr/hdp/3.0.1.0-187/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/usr/hdp/3.0.1.0-187/hive/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n20/05/06 16:04:06 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7.3.0.1.0-187\n20/05/06 16:04:07 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.\n20/05/06 16:04:07 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.\n20/05/06 16:04:07 INFO tool.CodeGenTool: Beginning code generation\nWed May 06 16:04:09 UTC 2020 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.\n20/05/06 16:04:10 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `santander_location` AS t LIMIT 1\n20/05/06 16:04:10 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `santander_location` AS t LIMIT 1\n20/05/06 16:04:10 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hdp/3.0.1.0-187/hadoop-mapreduce\n20/05/06 16:04:21 ERROR orm.CompilationManager: Could not rename /tmp/sqoop-zeppelin/compile/3b44b55aaf178195a40becbf6b2fe74a/santander_location.java to /home/zeppelin/./santander_location.java. Error: Destination '/home/zeppelin/./santander_location.java' already exists\n20/05/06 16:04:21 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-zeppelin/compile/3b44b55aaf178195a40becbf6b2fe74a/santander_location.jar\n20/05/06 16:04:21 WARN manager.MySQLManager: It looks like you are importing from mysql.\n20/05/06 16:04:21 WARN manager.MySQLManager: This transfer can be faster! Use the --direct\n20/05/06 16:04:21 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.\n20/05/06 16:04:21 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)\n20/05/06 16:04:21 INFO mapreduce.ImportJobBase: Beginning import of santander_location\n20/05/06 16:04:25 INFO client.RMProxy: Connecting to ResourceManager at sandbox-hdp.hortonworks.com/172.18.0.2:8050\n20/05/06 16:04:26 INFO client.AHSProxy: Connecting to Application History server at sandbox-hdp.hortonworks.com/172.18.0.2:10200\n20/05/06 16:04:28 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /user/zeppelin/.staging/job_1588774237075_0009\nWed May 06 16:04:37 UTC 2020 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.\n20/05/06 16:04:37 INFO db.DBInputFormat: Using read commited transaction isolation\n20/05/06 16:04:37 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`sensor`), MAX(`sensor`) FROM `santander_location`\n20/05/06 16:04:37 INFO db.IntegerSplitter: Split size: 1508; Num splits: 4 from: 1001 to: 7033\n20/05/06 16:04:37 INFO mapreduce.JobSubmitter: number of splits:4\n20/05/06 16:04:38 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1588774237075_0009\n20/05/06 16:04:38 INFO mapreduce.JobSubmitter: Executing with tokens: []\n20/05/06 16:04:39 INFO conf.Configuration: found resource resource-types.xml at file:/etc/hadoop/3.0.1.0-187/0/resource-types.xml\n20/05/06 16:04:39 INFO impl.YarnClientImpl: Submitted application application_1588774237075_0009\n20/05/06 16:04:39 INFO mapreduce.Job: The url to track the job: http://sandbox-hdp.hortonworks.com:8088/proxy/application_1588774237075_0009/\n20/05/06 16:04:39 INFO mapreduce.Job: Running job: job_1588774237075_0009\n"},{"type":"TEXT","data":"Paragraph received a SIGTERM\nExitValue: 143"}]},"apps":[],"jobName":"paragraph_1588780490442_1777300138","id":"20200506-155450_1862899336","dateCreated":"2020-05-06T15:54:50+0000","dateStarted":"2020-05-06T16:03:55+0000","dateFinished":"2020-05-06T16:04:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16775"},{"text":"%md\n\nVeréis que el trabajo queda lanzado y, posiblemente, dado que estamos trabajando en un Sandbox, si vais a [YARN](http://localhost:8088) veréis que se queda a la espera de que se liberen recursos para entrar a ejecutar su tarea.\n\nEsto se debe a que Spark sigue con su contexto lanzado y no ha liberado los recursos que hemos adquirido al empezar el notebook con nuestros procesos. Aunque parezca mentira, esto también os pasará en clusters de miles de nodos y tendréis que hacer un uso sensato de los recursos, liberándolos cuando no los necesiteis más.\n","user":"anonymous","dateUpdated":"2020-05-20T17:30:24+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Veréis que el trabajo queda lanzado y, posiblemente, dado que estamos trabajando en un Sandbox, si vais a <a href=\"http://localhost:8088\">YARN</a> veréis que se queda a la espera de que se liberen recursos para entrar a ejecutar su tarea.</p>\n<p>Esto se debe a que Spark sigue con su contexto lanzado y no ha liberado los recursos que hemos adquirido al empezar el notebook con nuestros procesos. Aunque parezca mentira, esto también os pasará en clusters de miles de nodos y tendréis que hacer un uso sensato de los recursos, liberándolos cuando no los necesiteis más.</p>\n"}]},"apps":[],"jobName":"paragraph_1588781359108_1705561885","id":"20200506-160919_1735015927","dateCreated":"2020-05-06T16:09:19+0000","dateStarted":"2020-05-20T17:30:24+0000","dateFinished":"2020-05-20T17:30:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16776"},{"text":"%spark\n\n// Con esto cerramos la sesión de Spark liberando los recursos para que progrese el job de Sqoop\nspark.close()\n","user":"anonymous","dateUpdated":"2020-05-06T16:12:04+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1588780970652_1466033661","id":"20200506-160250_536696337","dateCreated":"2020-05-06T16:02:50+0000","dateStarted":"2020-05-06T16:08:03+0000","dateFinished":"2020-05-06T16:08:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16777"},{"text":"%md\n\nCon esto Sqoop debería haber echado a andar y cuando indique que ha acabado podréis encontrar los datos en la ruta del HDFS indicada anteriormente","user":"anonymous","dateUpdated":"2020-05-06T16:13:18+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Con esto Sqoop debería haber echado a andar y cuando indique que ha acabado podréis encontrar los datos en la ruta del HDFS indicada anteriormente</p>\n"}]},"apps":[],"jobName":"paragraph_1588781543524_892443510","id":"20200506-161223_1619665090","dateCreated":"2020-05-06T16:12:23+0000","dateStarted":"2020-05-06T16:13:18+0000","dateFinished":"2020-05-06T16:13:18+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16778"},{"text":"%sh\n\nhdfs dfs -ls /tmp/mysql_import\n","user":"anonymous","dateUpdated":"2020-05-06T16:14:06+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Found 5 items\n-rw-r--r--   1 zeppelin hdfs          0 2020-05-06 16:09 /tmp/mysql_import/_SUCCESS\n-rw-r--r--   1 zeppelin hdfs       9723 2020-05-06 16:09 /tmp/mysql_import/part-m-00000\n-rw-r--r--   1 zeppelin hdfs       5570 2020-05-06 16:09 /tmp/mysql_import/part-m-00001\n-rw-r--r--   1 zeppelin hdfs       3294 2020-05-06 16:09 /tmp/mysql_import/part-m-00002\n-rw-r--r--   1 zeppelin hdfs       3695 2020-05-06 16:09 /tmp/mysql_import/part-m-00003\n"}]},"apps":[],"jobName":"paragraph_1588781283431_529010614","id":"20200506-160803_30483070","dateCreated":"2020-05-06T16:08:03+0000","dateStarted":"2020-05-06T16:14:07+0000","dateFinished":"2020-05-06T16:14:14+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16779"},{"text":"%md\n\nVeis que hay cuatro ficheros, cada uno con un trozo de la tabla original... y qué casualidad que por defecto hayan sido 4 los mappers que se han empleado en la tarea.","user":"anonymous","dateUpdated":"2020-05-20T17:31:11+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Veis que hay cuatro ficheros, cada uno con un trozo de la tabla original&hellip; y qué casualidad que por defecto hayan sido 4 los mappers que se han empleado en la tarea.</p>\n"}]},"apps":[],"jobName":"paragraph_1588781664180_-1066472037","id":"20200506-161424_159340037","dateCreated":"2020-05-06T16:14:24+0000","dateStarted":"2020-05-20T17:31:11+0000","dateFinished":"2020-05-20T17:31:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16780"},{"text":"%sh\n\nhdfs dfs -cat /tmp/mysql_import/part-m-00000","user":"anonymous","dateUpdated":"2020-05-06T16:15:42+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"4,1008,90,2,2020-05-03T15:40:00Z,-3.823525120182179,43.452711760164156,1008\n47,1042,1711,10,2020-05-03T15:40:00Z,-3.8146053033028666,43.456962663149746,1042\n3,1009,60,2,2020-05-03T15:40:00Z,-3.8191374490469063,43.45390483417005,1009\n18,2074,360,4,2020-05-03T15:40:00Z,-3.827108154680229,43.45597247555026,2074\n13,2009,240,3,2020-05-03T15:40:00Z,-3.8265408192255004,43.457606359291596,2009\n10,2075,180,3,2020-05-03T15:40:00Z,-3.8209273880449226,43.46004194609344,2075\n30,2018,518,11,2020-05-03T15:40:00Z,-3.8092391109650148,43.46072855348377,2018\n0,2076,0,0,2020-05-03T15:40:00Z,-3.7970621518703433,43.46253390831686,2076\n0,2077,0,0,2020-05-03T15:40:00Z,-3.796985924180725,43.46247140947698,2077\n0,2025,0,0,2020-05-03T15:40:00Z,-3.803739529984147,43.46184786070555,2025\n0,2023,0,0,2020-05-03T15:40:00Z,-3.8074409970886567,43.46150663162427,2023\n4,2036,60,0,2020-05-03T15:40:00Z,-3.796693593639584,43.466003135759806,2036\n5,1043,120,0,2020-05-03T15:40:00Z,-3.8308243536992626,43.45475696399685,1043\n5,2078,120,1,2020-05-03T15:40:00Z,-3.8025901425969377,43.46158580481061,2078\n4,2079,60,2,2020-05-03T15:40:00Z,-3.8024905448239057,43.46153247798179,2079\n0,1046,0,0,2020-05-03T15:40:00Z,-3.8108067237917993,43.45944784010991,1046\n0,1014,0,0,2020-05-03T15:40:00Z,-3.810756168340222,43.45945720289997,1014\n0,1047,0,0,2020-05-03T15:40:00Z,-3.810840883110269,43.45932153747705,1047\n0,1050,0,0,2020-05-03T15:40:00Z,-3.8102494922841634,43.45891153115453,1050\n0,1048,0,0,2020-05-03T15:40:00Z,-3.81047876356055,43.45858575092496,1048\n0,1049,0,0,2020-05-03T15:40:00Z,-3.8104000218102563,43.45861332197352,1049\n0,2003,0,0,2020-05-03T15:40:00Z,-3.8139800808915862,43.46158632893383,2003\n0,2002,0,0,2020-05-03T15:40:00Z,-3.813940350680939,43.461757693660225,2002\n0,2001,0,0,2020-05-03T15:40:00Z,-3.8139468932615173,43.46178466001913,2001\n0,2021,0,0,2020-05-03T15:40:00Z,-3.81401500509749,43.462162356179086,2021\n0,2022,0,0,2020-05-03T15:40:00Z,-3.814244640769943,43.462232755638496,2022\n0,2041,0,0,2020-05-03T15:40:00Z,-3.808462644711798,43.462570925639845,2041\n18,2042,134,10,2020-05-03T15:40:00Z,-3.8084127698357593,43.46272435153762,2042\n0,2044,0,0,2020-05-03T15:40:00Z,-3.809010557175584,43.462963239939384,2044\n0,2080,0,0,2020-05-03T15:40:00Z,-3.809459508342364,43.462608894523534,2080\n0,2081,0,0,2020-05-03T15:40:00Z,-3.809698582776827,43.46255317595502,2081\n0,1044,0,0,2020-05-03T15:40:00Z,-3.8344895151160396,43.45478430013786,1044\n5,1020,60,0,2020-05-03T15:40:00Z,-3.8338990088953167,43.454986703167904,1020\n0,1045,0,0,2020-05-03T15:40:00Z,-3.8338862787658297,43.45495978305075,1045\n5,2005,120,1,2020-05-03T15:40:00Z,-3.8291934054178505,43.45778524907514,2005\n6,2004,120,1,2020-05-03T15:40:00Z,-3.829453448183457,43.457819380270195,2004\n0,2039,0,0,2020-05-03T15:40:00Z,-3.8309692338260715,43.457484219321614,2039\n20,2007,240,12,2020-05-03T15:40:00Z,-3.826860775801198,43.45668560577261,2007\n7,2038,60,2,2020-05-03T15:40:00Z,-3.8272118437804736,43.45732237299824,2038\n5,2006,120,0,2020-05-03T15:40:00Z,-3.82669372781634,43.4573981546216,2006\n12,2008,180,5,2020-05-03T15:40:00Z,-3.8269361119328624,43.45849492837016,2008\n0,2010,0,0,2020-05-03T15:40:00Z,-3.822031545869677,43.46052024631393,2010\n10,2011,120,5,2020-05-03T15:40:00Z,-3.8221460811264554,43.460762539250624,2011\n51,1011,1768,11,2020-05-03T15:40:00Z,-3.8150191779287255,43.4550147809002,1011\n4,1029,60,2,2020-05-03T15:40:00Z,-3.806796796611148,43.459449191075585,1029\n0,2045,0,0,2020-05-03T15:40:00Z,-3.8298630556184774,43.45758229604125,2045\n2,2046,60,0,2020-05-03T15:40:00Z,-3.80930581618807,43.46146643497485,2046\n75,2012,480,70,2020-05-03T15:40:00Z,-3.823591450918023,43.4593294563689,2012\n0,2037,0,0,2020-05-03T15:40:00Z,-3.7954695353964687,43.463067249052074,2037\n0,2032,0,0,2020-05-03T15:40:00Z,-3.79358146194165,43.46284625642153,2032\n0,2033,0,0,2020-05-03T15:40:00Z,-3.7965489376875947,43.46336588126894,2033\n0,2034,0,0,2020-05-03T15:40:00Z,-3.796755697161408,43.46404876827053,2034\n0,2030,0,0,2020-05-03T15:40:00Z,-3.7983820118645273,43.46231760108342,2030\n0,2043,0,0,2020-05-03T15:40:00Z,-3.8006759663261986,43.4619143755439,2043\n4,2016,60,2,2020-05-03T15:40:00Z,-3.8108382488544974,43.461806746502354,2016\n4,2040,60,0,2020-05-03T15:40:00Z,-3.8118337178388275,43.46155656663748,2040\n28,2015,734,10,2020-05-03T15:40:00Z,-3.8123620473656814,43.46177792206734,2015\n7,2014,60,2,2020-05-03T15:40:00Z,-3.811675316244493,43.46209794982114,2014\n2,2017,60,0,2020-05-03T15:40:00Z,-3.810254302825353,43.461585778148,2017\n6,2019,171,2,2020-05-03T15:40:00Z,-3.8092627101024648,43.461385701260106,2019\n0,2029,0,0,2020-05-03T15:40:00Z,-3.7995263327061872,43.46257073037124,2029\n4,2028,120,1,2020-05-03T15:40:00Z,-3.799856082256003,43.46217223447628,2028\n5,2026,140,0,2020-05-03T15:40:00Z,-3.803732514814816,43.46178487983374,2026\n0,2013,0,0,2020-05-03T15:40:00Z,-3.820468915658142,43.460234327330596,2013\n6,2024,120,1,2020-05-03T15:40:00Z,-3.806259943588895,43.461559987147204,2024\n36,1010,1685,11,2020-05-03T15:40:00Z,-3.8176878393963745,43.455572004696336,1010\n53,1007,1901,30,2020-05-03T15:40:00Z,-3.8208669437179035,43.4544777123065,1007\n52,1019,2216,21,2020-05-03T15:40:00Z,-3.824337448790182,43.45355231312278,1019\n36,1013,1888,10,2020-05-03T15:40:00Z,-3.8129476349756692,43.45741567218863,1013\n1,1012,60,0,2020-05-03T15:40:00Z,-3.8124009290450944,43.45572674583399,1012\n0,1015,0,0,2020-05-03T15:40:00Z,-3.809519993050535,43.45769211107121,1015\n0,1016,0,0,2020-05-03T15:40:00Z,-3.8094464709396294,43.45764760997331,1016\n44,1018,1099,10,2020-05-03T15:40:00Z,-3.8088312418829293,43.45850737365815,1018\n4,1017,120,1,2020-05-03T15:40:00Z,-3.8075419750914095,43.45915579266607,1017\n10,1003,300,4,2020-05-03T15:40:00Z,-3.8283500834984032,43.45332521960427,1003\n7,1004,300,2,2020-05-03T15:40:00Z,-3.8282133940813843,43.45336222703308,1004\n36,1005,1194,10,2020-05-03T15:40:00Z,-3.828174157625682,43.45410986988515,1005\n5,1006,120,2,2020-05-03T15:40:00Z,-3.8282146888087834,43.4541816108836,1006\n4,1027,120,0,2020-05-03T15:40:00Z,-3.829655258781507,43.45368493421371,1027\n0,1001,0,0,2020-05-03T15:40:00Z,-3.8296248593691633,43.45363112885307,1001\n0,1002,0,0,2020-05-03T15:40:00Z,-3.8290158392261806,43.453383424964464,1002\n0,1028,0,0,2020-05-03T15:40:00Z,-3.8289546608952474,43.453428890108675,1028\n2,1022,60,0,2020-05-03T15:40:00Z,-3.8291177001725814,43.45106857527978,1022\n4,1023,60,2,2020-05-03T15:40:00Z,-3.8288686797610625,43.4511154022854,1023\n4,1024,60,0,2020-05-03T15:40:00Z,-3.8287997261102094,43.45104386749611,1024\n6,1021,120,2,2020-05-03T15:40:00Z,-3.829191474491337,43.45113107059377,1021\n0,1026,0,0,2020-05-03T15:40:00Z,-3.8268772017920365,43.45190419277687,1026\n0,1025,0,0,2020-05-03T15:40:00Z,-3.8265772807774594,43.45175328772584,1025\n0,2035,0,0,2020-05-03T15:40:00Z,-3.7967571941321827,43.4654804428202,2035\n10,2047,60,5,2020-05-03T15:40:00Z,-3.8218904857776446,43.46069234224635,2047\n86,2048,180,83,2020-05-03T15:40:00Z,-3.8222813266540117,43.46061749775413,2048\n7,2051,60,2,2020-05-03T15:40:00Z,-3.8113777117070837,43.46304551452138,2051\n0,2049,0,0,2020-05-03T15:40:00Z,-3.81111611466193,43.463173430838545,2049\n0,2050,0,0,2020-05-03T15:40:00Z,-3.8115503365183856,43.46319736269512,2050\n0,2055,0,0,2020-05-03T15:40:00Z,-3.811389576582701,43.46374776638193,2055\n0,2054,0,0,2020-05-03T15:40:00Z,-3.811118087882488,43.463875752882515,2054\n0,2053,0,0,2020-05-03T15:40:00Z,-3.810951883307577,43.46374186715641,2053\n0,2052,0,0,2020-05-03T15:40:00Z,-3.8109563462037066,43.46370581827247,2052\n99,2056,8,99,2020-05-03T15:40:00Z,-3.811210529401176,43.46357795492852,2056\n27,2057,447,10,2020-05-03T15:40:00Z,-3.8095678537811124,43.462112890409514,2057\n4,1031,60,2,2020-05-03T15:40:00Z,-3.8138738939220334,43.45532908536867,1031\n0,1032,0,0,2020-05-03T15:40:00Z,-3.832002144325709,43.45564883196645,1032\n4,1033,60,2,2020-05-03T15:40:00Z,-3.8318087200323983,43.45587534715788,1033\n3,1034,60,2,2020-05-03T15:40:00Z,-3.8317722489770314,43.45646089307195,1034\n5,1035,120,2,2020-05-03T15:40:00Z,-3.831913266506786,43.4565589146622,1035\n12,1039,120,2,2020-05-03T15:40:00Z,-3.8327635853780255,43.4565527249738,1039\n0,1038,0,0,2020-05-03T15:40:00Z,-3.8330100293854033,43.45640686080049,1038\n2,1037,60,0,2020-05-03T15:40:00Z,-3.8338238867380334,43.4564459504306,1037\n6,1036,120,2,2020-05-03T15:40:00Z,-3.83384798663933,43.45640075310374,1036\n0,1040,0,0,2020-05-03T15:40:00Z,-3.8350186673041358,43.45677039124234,1040\n4,1041,60,0,2020-05-03T15:40:00Z,-3.8350524060798388,43.45670711464303,1041\n0,1030,0,0,2020-05-03T15:40:00Z,-3.814929494279451,43.45487135097086,1030\n0,2058,0,0,2020-05-03T15:40:00Z,-3.817332008327677,43.46090510358437,2058\n0,2059,0,0,2020-05-03T15:40:00Z,-3.817493812890993,43.46135416243954,2059\n11,2060,92,5,2020-05-03T15:40:00Z,-3.8175875163592705,43.461794703682415,2060\n0,2061,0,0,2020-05-03T15:40:00Z,-3.817834481587112,43.46177492892027,2061\n0,2062,0,0,2020-05-03T15:40:00Z,-3.8180794821696455,43.461701141820086,2062\n0,2063,0,0,2020-05-03T15:40:00Z,-3.817846544840539,43.46147770039676,2063\n4,2064,60,2,2020-05-03T15:40:00Z,-3.817678413279169,43.461109726098826,2064\n6,2065,60,2,2020-05-03T15:40:00Z,-3.8177163885639422,43.4609923984819,2065\n0,2067,0,0,2020-05-03T15:40:00Z,-3.8175027006550426,43.46091288765064,2067\n0,2066,0,0,2020-05-03T15:40:00Z,-3.8176427577896654,43.46084885603933,2066\n0,2068,0,0,2020-05-03T15:40:00Z,-3.8175671633958963,43.46065130171458,2068\n0,2069,0,0,2020-05-03T15:40:00Z,-3.8171632051073225,43.46076224108783,2069\n24,2070,606,11,2020-05-03T15:40:00Z,-3.8088184654998183,43.45986711563261,2070\n10,2073,20,8,2020-05-03T15:40:00Z,-3.808672785157317,43.460066240929294,2073\n13,2072,110,4,2020-05-03T15:40:00Z,-3.808658589069818,43.460021319757836,2072\n54,2071,1380,9,2020-05-03T15:40:00Z,-3.809061922742889,43.459955436401316,2071\n"}]},"apps":[],"jobName":"paragraph_1588781616704_276234526","id":"20200506-161336_1500930704","dateCreated":"2020-05-06T16:13:36+0000","dateStarted":"2020-05-06T16:15:42+0000","dateFinished":"2020-05-06T16:15:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16781"},{"text":"%md\n\nComo veis lo que nos hemos traido no es más que la tabla en formato CSV. Y no importa demasiado cuantos TeraBytes ocupe la tabla original, podemos importarla dividiendo en muchos trozos la tarea. Como solía decir Julio Cesar antiguamente: *divide et impera*\n\nDe hecho, lo interesante sería poder traernos los datos y los metadatos también. Podríamos guardar estos metadatos en Hive y así solo tener que lanzar consultas SQL para procesar estos datos de nuevo en nuestro entorno Hadoop.\n\nProbablemente para realizar esta acción necesitéis entrar en Ranger y darle permisos a Hive sobre la ruta /user/zeppelin donde Sqoop dejará los datos temporales.","user":"anonymous","dateUpdated":"2020-05-20T17:34:23+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Como veis lo que nos hemos traido no es más que la tabla en formato CSV. Y no importa demasiado cuantos TeraBytes ocupe la tabla original, podemos importarla dividiendo en muchos trozos la tarea. Como solía decir Julio Cesar antiguamente: <em>divide et impera</em></p>\n<p>De hecho, lo interesante sería poder traernos los datos y los metadatos también. Podríamos guardar estos metadatos en Hive y así solo tener que lanzar consultas SQL para procesar estos datos de nuevo en nuestro entorno Hadoop.</p>\n<p>Probablemente para realizar esta acción necesitéis entrar en Ranger y darle permisos a Hive sobre la ruta /user/zeppelin donde Sqoop dejará los datos temporales.</p>\n"}]},"apps":[],"jobName":"paragraph_1588781742323_693500564","id":"20200506-161542_186198840","dateCreated":"2020-05-06T16:15:42+0000","dateStarted":"2020-05-20T17:34:23+0000","dateFinished":"2020-05-20T17:34:23+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16782"},{"text":"%sh\n\n# Y volvemos a importar la tabla pero ahora importando los metadatos a Hive\n# -m 1 es para indicar que solo emplearemso un mapper, dado que la tabla no es muy grande en este caso\nsqoop import --connect jdbc:mysql://localhost:3306/ranger \\\n--username root \\\n--password hortonworks1 \\\n--table \"santander_location\" \\\n--hive-import --hive-table \"santander_location\" -m 1","user":"anonymous","dateUpdated":"2020-05-06T17:30:48+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1588781888791_2119782826","id":"20200506-161808_756278392","dateCreated":"2020-05-06T16:18:08+0000","dateStarted":"2020-05-06T17:28:59+0000","dateFinished":"2020-05-06T17:30:00+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16783"},{"text":"%md\n\nSi os habeis animado con la útlima parte habréis podido ver que generando un nuevo párrafo del tipo %jdbc(hive) la tabla traída desde MySQL ahora es una tabla Hive más con lo que podéis lanzar los procesos desde Spark a la base de datos directamente o traeros la información al entorno Hadoop. Queda a vuestro criterio saber cuál es la mejor alternativa... aunque no serán las únicas que veremos.\n\nA lo largo de estos notebooks hemos visto cómo:\n* Traernos datos puntuales desde CSVs y trasformarlos en tablas\n* Cómo podemos usar Spark para juntar esos datos y procesar volumenes en distribuído, aunque la información no sea originalmente tabular\n* Cómo interactuar con fuentes relacionales tanto en lectura como escritura\n\nEsto nos permite leer datos de multitud de fuentes, procesarlas al margen del volúmen que supongan y devolver resultados a los sistemas informacionales más usuales. ¿Pero creeis que en le mundo Big Data somos los humanos los que lanzamos estas taréas? Ni siquiera esperamos a juntar un volumen de datos grande para procesarlo, esto se suele realizar tan pronto como los datos estén disponibles... como parte del *stream* de datos.","user":"anonymous","dateUpdated":"2020-05-20T17:38:18+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Si os habeis animado con la útlima parte habréis podido ver que generando un nuevo párrafo del tipo %jdbc(hive) la tabla traída desde MySQL ahora es una tabla Hive más con lo que podéis lanzar los procesos desde Spark a la base de datos directamente o traeros la información al entorno Hadoop. Queda a vuestro criterio saber cuál es la mejor alternativa&hellip; aunque no serán las únicas que veremos.</p>\n<p>A lo largo de estos notebooks hemos visto cómo:</p>\n<ul>\n<li>Traernos datos puntuales desde CSVs y trasformarlos en tablas</li>\n<li>Cómo podemos usar Spark para juntar esos datos y procesar volumenes en distribuído, aunque la información no sea originalmente tabular</li>\n<li>Cómo interactuar con fuentes relacionales tanto en lectura como escritura</li>\n</ul>\n<p>Esto nos permite leer datos de multitud de fuentes, procesarlas al margen del volúmen que supongan y devolver resultados a los sistemas informacionales más usuales. ¿Pero creeis que en le mundo Big Data somos los humanos los que lanzamos estas taréas? Ni siquiera esperamos a juntar un volumen de datos grande para procesarlo, esto se suele realizar tan pronto como los datos estén disponibles&hellip; como parte del <em>stream</em> de datos.</p>\n"}]},"apps":[],"jobName":"paragraph_1588782208405_1074564567","id":"20200506-162328_1098275775","dateCreated":"2020-05-06T16:23:28+0000","dateStarted":"2020-05-20T17:38:18+0000","dateFinished":"2020-05-20T17:38:18+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16784"}],"name":"Obtención de fuentes de datos y enriquecimiento","id":"2F823CYBV","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"sh:shared_process":[],"jdbc:shared_process":[],"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}