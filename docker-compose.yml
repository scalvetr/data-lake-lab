version: "3.5"
services:

  zookeeper:
    image: zookeeper:3.6.2
    hostname: zookeeper
    container_name: big-data-lab-zookeeper
    expose:
      - 2181
    volumes:
      #- zookeeper-logs:/logs
      - ./ws/zookeeper/logs:/logs
      #- zookeeper-data:/data
    networks:
      big-data-lab-net:
        ipv4_address: 172.27.1.5

  kafka:
    image: wurstmeister/kafka:2.13-2.6.0
    hostname: kafka
    container_name: big-data-lab-kafka
    command: [start-kafka.sh]
    ports:
      - "9092:9092"
    environment:
      KAFKA_ADVERTISED_LISTENERS: INSIDE://:9092
      KAFKA_LISTENERS: INSIDE://:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
    volumes:
      - ./ws/kafka/logs:/kafka/logs
      #- kafka-logs:/kafka/logs
    networks:
      big-data-lab-net:
        ipv4_address: 172.27.1.6
    depends_on:
      - zookeeper

  nifi:
    image: apache/nifi:1.12.1
    hostname: nifi
    container_name: big-data-lab-nifi
    environment:
      NIFI_WEB_HTTP_PORT: 9081
      NIFI_WEB_HTTPS_PORT: 9443
    ports:
      - "9081:9081"
      - "9443:9443"
    volumes:
      - ./ws/nifi/logs:/opt/nifi/nifi-current/logs
      - ./ws/shared:/opt/workspace
      #- nifi-logs:/opt/nifi/nifi-current/logs
      #- shared-workspace:/opt/workspace
    depends_on:
      - zookeeper
    networks:
      big-data-lab-net:
        ipv4_address: 172.27.1.10


  hdfs-namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8
    hostname: hdfs-namenode
    container_name: big-data-lab-hdfs-namenode
    volumes:
      - ./ws/hadoop/namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=big-data-lab
    ports:
      - "50070:50070"
    networks:
      big-data-lab-net:
        ipv4_address: 172.27.1.11


  hdfs-datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8
    hostname: hdfs-datanode
    container_name: big-data-lab-hdfs-datanode
    volumes:
      - ./ws/hadoop/datanode:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "hdfs-namenode:50070"
      CORE_CONF_fs_defaultFS: "hdfs://hdfs-namenode:8020"
    depends_on:
      - hdfs-namenode
    ports:
      - "50075:50075"
    networks:
      big-data-lab-net:
        ipv4_address: 172.27.1.12


  spark-master:
    image: bde2020/spark-master:2.4.0-hadoop2.7
    hostname: spark-master
    container_name: big-data-lab-spark-master
    ports:
      - 8080:8080
      - 7077:7077
    environment:
      CORE_CONF_fs_defaultFS: "hdfs://hdfs-namenode:8020"
    networks:
      big-data-lab-net:
        ipv4_address: 172.27.1.13

  spark-worker:
    image: bde2020/spark-worker:2.4.0-hadoop2.7
    hostname: spark-worker
    container_name: big-data-lab-spark-worker
    depends_on:
      - spark-master
    environment:
      SPARK_MASTER: "spark://spark-master:7077"
      CORE_CONF_fs_defaultFS: "hdfs://hdfs-namenode:8020"
    ports:
      - 8081:8081
    networks:
      big-data-lab-net:
        ipv4_address: 172.27.1.14

  hive-server:
    image: bde2020/hive:2.3.2
    hostname: hive-server
    container_name: big-data-lab-hive-server
    environment:
      CORE_CONF_fs_defaultFS: "hdfs://hdfs-namenode:8020"
      CORE_CONF_hadoop_http_staticuser_user: "root"
      HDFS_CONF_dfs_webhdfs_enabled: "true"
      HDFS_CONF_dfs_permissions_enabled: "false"
      HIVE_SITE_CONF_hive_metastore_warehouse_dir: "/warehouse/tablespace/managed"
    depends_on:
      - hdfs-datanode
    ports:
      - "10000:10000"
    networks:
      big-data-lab-net:
        ipv4_address: 172.27.1.15

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.10.1
    hostname: elasticsearch
    container_name: big-data-lab-elasticsearch
    volumes:
      - ./ws/elasticsearch/logs:/usr/share/elasticsearch/logs/
    environment:
      discovery.type: single-node
    ports:
      - "9200:9200"
      - "9300:9300"
    networks:
      big-data-lab-net:
        ipv4_address: 172.27.1.21

  kibana:
    image: docker.elastic.co/kibana/kibana:7.10.1
    hostname: kibana
    container_name: big-data-lab-kibana
    ports:
      - "5601:5601"
    environment:
      ELASTICSEARCH_HOSTS: "http://elasticsearch:9200"
    networks:
      big-data-lab-net:
        ipv4_address: 172.27.1.22
    depends_on:
      - elasticsearch

  mysql:
    image: mysql:8.0.22
    hostname: mysql
    container_name: big-data-lab-mysql
    ports:
      - "3306:3306"
    environment:
      MYSQL_USER: "big-data-user"
      MYSQL_PASSWORD: "big-data-user"
      MYSQL_ROOT_PASSWORD: "root"
    networks:
      big-data-lab-net:
        ipv4_address: 172.27.1.30

  zeppelin:
    build:
      context: docker/zeppelin/
      labels:
        - scalvetr/spark
    hostname: zeppelin
    container_name: big-data-lab-zeppelin
    ports:
      - "9080:8080"
    environment:
      ZEPPELIN_LOG_DIR: /zeppelin/logs
      ZEPPELIN_NOTEBOOK_DIR: /zeppelin/notebook
      HADOOP_USER_NAME: root
      SPARK_SUBMIT_OPTIONS: "--files /opt/workspace/jdbs/mysql-connector-java.jar,/opt/workspace/jdbs/postgress.jar"
    volumes:
      - ./ws/shared:/opt/workspace
      - ./ws/zeppelin/notebooks:/zeppelin/notebook
      - ./ws/zeppelin/logs:/zeppelin/logs
      #- ./ws/zeppelin/conf/interpreter.json:/zeppelin/conf/interpreter.json:rw
      - ./ws/zeppelin/conf/log4j.properties:/zeppelin/conf/log4j.properties
      - ./ws/zeppelin/conf/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml
      - ./ws/zeppelin/conf/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml
      - ./ws/zeppelin/conf/hive-site.xml:/opt/hadoop/etc/hadoop/hive-site.xml
      #- shared-workspace:/opt/workspace
      #- zeppelin-notebooks:/zeppelin/notebook
      #- zeppelin-logs:/zeppelin/logs
    networks:
      big-data-lab-net:
        ipv4_address: 172.27.1.4

networks:
  big-data-lab-net:
    name: "big-data-lab-net"
    ipam:
      driver: default
      config:
        - subnet: 172.27.0.0/16

volumes:
  zookeeper-data:
    name: "big-data-lab-zookeeper-data"
    driver: local
