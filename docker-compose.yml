version: "3.5"
services:

  zookeeper:
    image: zookeeper:3.6.2
    hostname: zookeeper
    container_name: big-data-lab-zookeeper
    expose:
      - 2181
    volumes:
      #- zookeeper-logs:/logs
      - ./ws/zookeeper/logs:/logs
      #- zookeeper-data:/data
    networks:
      big-data-lab-net:
        ipv4_address: 172.27.1.5

  kafka:
    image: wurstmeister/kafka:2.13-2.6.0
    hostname: kafka
    container_name: big-data-lab-kafka
    command: [start-kafka.sh]
    ports:
      - "9092:9092"
    environment:
      KAFKA_ADVERTISED_LISTENERS: "INSIDE://kafka:29092,OUTSIDE://172.27.1.6:9092"
      KAFKA_LISTENERS: INSIDE://:29092,OUTSIDE://:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: OUTSIDE
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
    volumes:
      - ./ws/shared:/opt/workspace
      - ./ws/kafka/logs:/opt/kafka/logs
      #- kafka-logs:/kafka/logs
    networks:
      big-data-lab-net:
        ipv4_address: 172.27.1.6
    depends_on:
      - zookeeper

  nifi:
    image: apache/nifi:1.12.1
    hostname: nifi
    container_name: big-data-lab-nifi
    environment:
      NIFI_WEB_HTTP_PORT: 9081
      NIFI_WEB_HTTPS_PORT: 9443
    ports:
      - "9081:9081"
      - "9443:9443"
    volumes:
      - ./ws/shared:/opt/workspace
      - ./ws/nifi/logs:/opt/nifi/nifi-current/logs
      - ./ws/nifi/state:/data/nifi/state
      #- nifi-logs:/opt/nifi/nifi-current/logs
      #- shared-workspace:/opt/workspace
    depends_on:
      - zookeeper
    networks:
      big-data-lab-net:
        ipv4_address: 172.27.1.10


  hdfs-namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8
    hostname: hdfs-namenode
    container_name: big-data-lab-hdfs-namenode
    volumes:
      - ./ws/shared:/opt/workspace
      - ./ws/hadoop/namenode/name:/hadoop/dfs/name
      - ./ws/hadoop/namenode/logs:/opt/hadoop-2.7.4/logs
    environment:
      - CLUSTER_NAME=big-data-lab
    ports:
      - "50070:50070"
    networks:
      big-data-lab-net:
        ipv4_address: 172.27.1.11


  hdfs-datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8
    hostname: hdfs-datanode
    container_name: big-data-lab-hdfs-datanode
    volumes:
      - ./ws/shared:/opt/workspace
      - ./ws/hadoop/datanode/data:/hadoop/dfs/data
      - ./ws/hadoop/datanode/logs:/opt/hadoop-2.7.4/logs
    environment:
      SERVICE_PRECONDITION: "hdfs-namenode:50070"
      CORE_CONF_fs_defaultFS: "hdfs://hdfs-namenode:8020"
    depends_on:
      - hdfs-namenode
    ports:
      - "50075:50075"
    networks:
      big-data-lab-net:
        ipv4_address: 172.27.1.12


  spark-master:
    image: bde2020/spark-master:2.4.0-hadoop2.7
    hostname: spark-master
    container_name: big-data-lab-spark-master
    volumes:
      - ./ws/shared:/opt/workspace
      #- ./ws/spark/master/conf:/spark/conf
      - ./ws/spark/master/logs:/spark/logs
    ports:
      - 8080:8080
      - 7077:7077
    environment:
      CORE_CONF_fs_defaultFS: "hdfs://hdfs-namenode:8020"
    networks:
      big-data-lab-net:
        ipv4_address: 172.27.1.13

  spark-worker:
    image: bde2020/spark-worker:2.4.0-hadoop2.7
    hostname: spark-worker
    container_name: big-data-lab-spark-worker
    volumes:
      - ./ws/shared:/opt/workspace
      #- ./ws/spark/worker/conf:/spark/conf
      - ./ws/spark/worker/logs:/spark/logs
    depends_on:
      - spark-master
    environment:
      SPARK_MASTER: "spark://spark-master:7077"
      CORE_CONF_fs_defaultFS: "hdfs://hdfs-namenode:8020"
    ports:
      - 8081:8081
    networks:
      big-data-lab-net:
        ipv4_address: 172.27.1.14

  hive:
    build:
      context: docker/hive/
      labels:
        - scalvetr/hive
    hostname: hive
    container_name: big-data-lab-hive
    volumes:
      - ./ws/shared:/opt/workspace
      - ./ws/hive/data:/opt/derby-data
    environment:
      CORE_CONF_fs_defaultFS: "hdfs://hdfs-namenode:8020"
      CORE_CONF_hadoop_http_staticuser_user: "root"
      HDFS_CONF_dfs_webhdfs_enabled: "true"
      HDFS_CONF_dfs_permissions_enabled: "false"
      HIVE_PORT: "10000"
      HIVE_SITE_CONF_hive_metastore_warehouse_dir: "hdfs://hdfs-namenode:8020/warehouse/tablespace/managed"
      HIVE_SITE_CONF_javax_jdo_option_ConnectionURL: "jdbc:derby://localhost:1527/metastore_db;create=true"
      HIVE_SITE_CONF_javax_jdo_option_ConnectionDriverName: "org.apache.derby.jdbc.ClientDriver"
      HIVE_SITE_CONF_datanucleus_autoCreateSchema: "false"
      HIVE_SITE_CONF_hive_metastore_uris: "thrift://localhost:9083"
      HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check: "false"
    depends_on:
      - hdfs-datanode
    ports:
      - "9083:9083"
      - "10000:10000"
      - "10001:10001"
      - "10002:10002"
    networks:
      big-data-lab-net:
        ipv4_address: 172.27.1.15

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.10.1
    hostname: elasticsearch
    container_name: big-data-lab-elasticsearch
    volumes:
      - ./ws/shared:/opt/workspace
      - ./ws/elasticsearch/logs:/usr/share/elasticsearch/logs
    environment:
      discovery.type: single-node
    ports:
      - "9200:9200"
      - "9300:9300"
    networks:
      big-data-lab-net:
        ipv4_address: 172.27.1.21

  kibana:
    image: docker.elastic.co/kibana/kibana:7.10.1
    hostname: kibana
    container_name: big-data-lab-kibana
    volumes:
      - ./ws/shared:/opt/workspace
      - ./ws/kibana/logs:/opt/kibana/logs
    ports:
      - "5601:5601"
    environment:
      ELASTICSEARCH_HOSTS: "http://elasticsearch:9200"
    networks:
      big-data-lab-net:
        ipv4_address: 172.27.1.22
    depends_on:
      - elasticsearch

  mysql:
    image: mysql:8.0.22
    hostname: mysql
    container_name: big-data-lab-mysql
    ports:
      - "3306:3306"
    environment:
      MYSQL_USER: "big-data-user"
      MYSQL_PASSWORD: "big-data-user"
      MYSQL_ROOT_PASSWORD: "hortonworks1"
      MYSQL_DATABASE: "ranger"
    volumes:
      - ./ws/mysql/data:/var/lib/mysql
      - ./ws/mysql/logs:/var/log/mysql
      - ./ws/mysql/conf:/etc/mysql/conf.d
    networks:
      big-data-lab-net:
        ipv4_address: 172.27.1.30

  zeppelin:
    build:
      context: docker/zeppelin/
      labels:
        - scalvetr/zeppelin
    hostname: zeppelin
    container_name: big-data-lab-zeppelin
    ports:
      - "9080:8080"
    environment:
      ZEPPELIN_LOG_DIR: /zeppelin/logs
      ZEPPELIN_NOTEBOOK_DIR: /zeppelin/notebook
      HADOOP_USER_NAME: root
      SPARK_SUBMIT_OPTIONS: "--jars /opt/workspace/jdbc/mysql-connector-java.jar,/opt/workspace/jdbc/postgresql.jar,/opt/workspace/jdbc/hive-jdbc.jar"
    volumes:
      - ./ws/shared:/opt/workspace
      - ./ws/zeppelin/notebooks:/zeppelin/notebook
      - ./ws/zeppelin/logs:/zeppelin/logs
      #- ./ws/zeppelin/conf/interpreter.json:/zeppelin/conf/interpreter.json
      - ./ws/shared/jdbc/mysql-connector-java.jar:/opt/sqoop/lib/mysql-connector-java.jar
      - ./ws/shared/jdbc/postgresql.jar:/opt/sqoop/lib/postgresql.jar
      - ./ws/zeppelin/conf/log4j.properties:/zeppelin/conf/log4j.properties
      - ./ws/zeppelin/conf/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml
      - ./ws/zeppelin/conf/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml
      - ./ws/zeppelin/conf/hive-site.xml:/opt/hadoop/etc/hadoop/hive-site.xml
      #- shared-workspace:/opt/workspace
      #- zeppelin-notebooks:/zeppelin/notebook
      #- zeppelin-logs:/zeppelin/logs
    networks:
      big-data-lab-net:
        ipv4_address: 172.27.1.4

networks:
  big-data-lab-net:
    name: "big-data-lab-net"
    ipam:
      driver: default
      config:
        - subnet: 172.27.0.0/16

volumes:
  zookeeper-data:
    name: "big-data-lab-zookeeper-data"
    driver: local
